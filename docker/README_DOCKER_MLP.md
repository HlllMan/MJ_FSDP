# MLP è®­ç»ƒ Docker å¯åŠ¨æŒ‡å—

## ğŸ¯ é…ç½®æ¦‚è§ˆ

- **èŠ‚ç‚¹æ•°é‡**: 2 ä¸ªèŠ‚ç‚¹ï¼ˆDGX-011, DGX-092ï¼‰
- **æ¯èŠ‚ç‚¹ GPU**: 8 å¼ å¡
- **æ€» GPU**: 16 å¼ å¡
- **TP_SIZE**: 2ï¼ˆTensor Parallelismï¼‰
- **DP_SIZE**: 8ï¼ˆData Parallelismï¼‰
- **ä»»åŠ¡**: MLP è®­ç»ƒï¼ˆTP.pyï¼‰
- **Docker é•œåƒ**: `nvcr.io/nvidia/pytorch:24.01-py3`ï¼ˆPyTorch 2.2+ é¢„è£…ï¼Œæ— éœ€å‡çº§ï¼‰

## ğŸ“Š GPU æ‹“æ‰‘ç»“æ„

```
æ€»å…± 16 å¼  GPUï¼Œåˆ†ç»„å¦‚ä¸‹ï¼š

DP Group 0: Node0-GPU0, Node0-GPU1 (TP)
DP Group 1: Node0-GPU2, Node0-GPU3 (TP)
DP Group 2: Node0-GPU4, Node0-GPU5 (TP)
DP Group 3: Node0-GPU6, Node0-GPU7 (TP)
DP Group 4: Node1-GPU0, Node1-GPU1 (TP)
DP Group 5: Node1-GPU2, Node1-GPU3 (TP)
DP Group 6: Node1-GPU4, Node1-GPU5 (TP)
DP Group 7: Node1-GPU6, Node1-GPU7 (TP)

æ¯ä¸ª DP Group å†…çš„ 2 å¼ å¡é€šè¿‡ TP åˆ‡åˆ†æ¨¡å‹
8 ä¸ª DP Group å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡
```

## ğŸš€ å¯åŠ¨æ­¥éª¤

### **1. ç¡®å®š Master èŠ‚ç‚¹çš„ IP åœ°å€**

åœ¨ DGX-011 (Master èŠ‚ç‚¹) ä¸Šï¼š

```bash
# æŸ¥çœ‹ IP åœ°å€
hostname -I
# æˆ–
ip addr show | grep "inet " | grep -v 127.0.0.1
```

å‡è®¾ Master çš„ IP æ˜¯: `192.168.1.100`

### **2. å¯åŠ¨ Master èŠ‚ç‚¹ (DGX-011)**

```bash
cd /public/data0/HOME/jdnlp1004/miaoji.norman/FSDP/final

# å¯åŠ¨ Node 0ï¼ˆMasterï¼‰
bash run_docker_mlp.sh 0 192.168.1.100 29600
```

å‚æ•°è¯´æ˜ï¼š
- `0`: node_rankï¼ˆMaster èŠ‚ç‚¹ï¼‰
- `192.168.1.100`: master_addrï¼ˆMaster çš„ IPï¼‰
- `29600`: master_portï¼ˆé€šä¿¡ç«¯å£ï¼‰

### **3. å¯åŠ¨ Worker èŠ‚ç‚¹ (DGX-092)**

åœ¨ **DGX-092** ä¸Šæ‰§è¡Œï¼š

```bash
cd /public/data0/HOME/jdnlp1004/miaoji.norman/FSDP/final

# å¯åŠ¨ Node 1ï¼ˆWorkerï¼‰
bash run_docker_mlp.sh 1 192.168.1.100 29600
```

å‚æ•°è¯´æ˜ï¼š
- `1`: node_rankï¼ˆWorker èŠ‚ç‚¹ï¼‰
- `192.168.1.100`: master_addrï¼ˆ**å’Œ Master ä¸€æ ·ï¼Œç”¨ Master çš„ IP**ï¼‰
- `29600`: master_portï¼ˆ**å’Œ Master ä¸€æ ·**ï¼‰

## âœ… éªŒè¯å¯åŠ¨æˆåŠŸ

### **æ­£å¸¸è¾“å‡ºåº”è¯¥åŒ…å«**ï¼š

1. **åˆå§‹åŒ–ä¿¡æ¯**ï¼š
```
[Rank 0] DP rank: 0, TP mesh: DeviceMesh(...), DP mesh: DeviceMesh(...)
[Rank 1] DP rank: 0, TP mesh: DeviceMesh(...), DP mesh: DeviceMesh(...)
...
[Rank 15] DP rank: 7, TP mesh: DeviceMesh(...), DP mesh: DeviceMesh(...)
```

2. **æ‰€æœ‰ 16 ä¸ª rank éƒ½å‡ºç°**ï¼š
- Rank 0-7: Node 0 çš„ 8 å¼ å¡
- Rank 8-15: Node 1 çš„ 8 å¼ å¡

3. **è®­ç»ƒå¼€å§‹**ï¼š
```
[Rank 0] Starting epoch 0
[Rank 0] Processing first batch of epoch 0
[Rank 0] Starting forward pass...
[Rank 0] Loss computed: 1.234567
```

## ğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥

### **é—®é¢˜ 1: Worker èŠ‚ç‚¹è¿æ¥è¶…æ—¶**

```
Error: Timed out initializing process group in store based barrier
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ Master IP æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥ç«¯å£ 29600 æ˜¯å¦è¢«å ç”¨ï¼š`netstat -tulpn | grep 29600`
3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
4. ç¡®ä¿ Master èŠ‚ç‚¹å…ˆå¯åŠ¨

### **é—®é¢˜ 2: ç«¯å£è¢«å ç”¨**

```
Address already in use
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# åœæ­¢æ‰€æœ‰ pytorch Docker å®¹å™¨
docker stop $(docker ps -q --filter ancestor=nvcr.io/nvidia/pytorch:24.01-py3)

# æˆ–è€…æ¢ä¸€ä¸ªç«¯å£ï¼ˆå¦‚ 29700ï¼‰
bash run_docker_mlp.sh 0 192.168.1.100 29700
```

### **é—®é¢˜ 3: CUDA åˆå§‹åŒ–å¤±è´¥**

```
CUDA failed to initialize
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- è„šæœ¬å·²ç»åŒ…å« `--privileged` ç­‰å¿…è¦å‚æ•°
- æ£€æŸ¥ `nvidia-smi` æ˜¯å¦æ­£å¸¸
- é‡å¯ Docker daemon

### **é—®é¢˜ 4: NCCL é€šä¿¡é”™è¯¯**

```
NCCL error: unhandled system error
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ InfiniBand è¿æ¥ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
- è„šæœ¬å·²ç»è®¾ç½®äº† NCCL ä¼˜åŒ–å‚æ•°
- æŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼šä¿®æ”¹ `launch_mlp.sh` ä¸­ `NCCL_DEBUG=INFO`

## ğŸ“ æ–‡ä»¶è¯´æ˜

- **run_docker_mlp.sh**: Docker å¯åŠ¨è„šæœ¬ï¼ˆåœ¨å®¿ä¸»æœºè¿è¡Œï¼‰
- **launch_mlp.sh**: å®¹å™¨å†…å¯åŠ¨è„šæœ¬ï¼ˆè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯åŠ¨ torchrunï¼‰
- **TP.py**: ä¸»è®­ç»ƒè„šæœ¬ï¼ˆå·²é…ç½® TP=2ï¼‰
- **TP_parallel.py**: TP å¹¶è¡Œå®ç°
- **DDP.py**: è‡ªå®šä¹‰ DDP å®ç°
- **MyTrainDataset.py**: æ•°æ®é›†å’Œæ¨¡å‹å®šä¹‰

## ğŸ“ å¦‚ä½•ä¿®æ”¹é…ç½®

### **ä¿®æ”¹ TP_SIZE**

åœ¨ `TP.py` ä¸­ä¿®æ”¹ï¼š

```python
tp_size = 2  # æ”¹æˆå…¶ä»–å€¼ï¼Œå¦‚ 4
```

æ³¨æ„ï¼š`world_size` å¿…é¡»èƒ½è¢« `tp_size` æ•´é™¤

### **ä¿®æ”¹ batch_size æˆ– epochs**

åœ¨ `TP.py` çš„æœ€åï¼š

```python
if __name__ == "__main__":
    main(total_epochs=10, batch_size=32)  # ä¿®æ”¹è¿™é‡Œ
```

### **ä¿®æ”¹ NCCL æ—¥å¿—çº§åˆ«**

åœ¨ `launch_mlp.sh` ä¸­ä¿®æ”¹ï¼š

```bash
export NCCL_DEBUG=WARN  # æ”¹æˆ INFO æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
```

## ğŸ›‘ åœæ­¢è®­ç»ƒ

```bash
# åœæ­¢æ‰€æœ‰ pytorch Docker å®¹å™¨
docker stop $(docker ps -q --filter ancestor=nvcr.io/nvidia/pytorch:24.01-py3)

# æˆ–è€…åœ¨è¿è¡Œçš„ç»ˆç«¯æŒ‰ Ctrl+C
```

## ğŸ†š é•œåƒæ›´æ–°è¯´æ˜

**æ—§ç‰ˆæœ¬ï¼ˆ23.10-py3ï¼‰**ï¼š
- æ¯æ¬¡å¯åŠ¨éœ€è¦å‡çº§ PyTorchï¼ˆè€—æ—¶ 2-5 åˆ†é’Ÿï¼‰
- éœ€è¦ç½‘ç»œè¿æ¥ä¸‹è½½åŒ…

**æ–°ç‰ˆæœ¬ï¼ˆ24.01-py3ï¼‰**ï¼š
- PyTorch 2.2+ å·²é¢„è£…ï¼Œå¯åŠ¨é€Ÿåº¦å¿«
- æ— éœ€ç½‘ç»œå‡çº§ï¼Œç¦»çº¿å¯ç”¨
- ä¸å•å¡åŸºå‡†ç‰ˆæœ¬ä½¿ç”¨ç›¸åŒé•œåƒï¼Œä¾¿äºéªŒè¯

## ğŸ“ å¿«é€Ÿå¯åŠ¨å‘½ä»¤ï¼ˆå¤åˆ¶ç²˜è´´ï¼‰

### DGX-011 (Master):
```bash
cd /public/data0/HOME/jdnlp1004/miaoji.norman/FSDP/final
bash run_docker_mlp.sh 0 $(hostname -I | awk '{print $1}') 29600
```

### DGX-092 (Worker):
```bash
cd /public/data0/HOME/jdnlp1004/miaoji.norman/FSDP/final
# æ›¿æ¢ä¸‹é¢çš„ MASTER_IP ä¸ºå®é™…çš„ Master IP
bash run_docker_mlp.sh 1 MASTER_IP 29600
```

## ğŸ‰ æˆåŠŸæ ‡å¿—

çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºè¯´æ˜å¯åŠ¨æˆåŠŸï¼š
- âœ… æ‰€æœ‰ 16 ä¸ª rank éƒ½æ‰“å°äº†åˆå§‹åŒ–ä¿¡æ¯
- âœ… DP rank ä» 0 åˆ° 7 éƒ½å­˜åœ¨
- âœ… å¼€å§‹æ‰“å° "Starting epoch 0"
- âœ… å¼€å§‹æ‰“å° Loss å€¼


